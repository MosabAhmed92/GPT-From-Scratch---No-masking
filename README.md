# GPT-From-Scratch---No-masking
A minimal implementation of GPT (Generative Pre-trained Transformer) in PyTorch for educational purposes.


# Overview
This project implements the core architecture of GPT to understand how transformers work. It builds the model from the ground up, starting with the attention mechanism and working up to the full GPT model.

# Limitations
This is an educational implementation. It does not include:

- Causal masking (preventing attention to future tokens)
- Dropout for regularization
- Training loop and loss function
- Text tokenization (converting text to token IDs)
- Text generation (sampling from output probabilities)


